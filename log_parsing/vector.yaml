sources:
  kafka_logs:
    type: "kafka"
    bootstrap_servers: "kafka:9092"
    group_id: "vector-clickhouse-consumer"
    topics:
      - "logs"
    session_timeout_ms: 30000
    fetch_wait_max_ms: 500
    auto_offset_reset: "earliest"
  internal_metrics:
    type: "internal_metrics"

transforms:
  parse_kafka_message:
    inputs:
      - "kafka_logs"
    type: "remap"
    source: |
      parsed_message, err = parse_json(.message)
      . = parsed_message

  route_logs:
    inputs:
      - "parse_kafka_message"
    type: "route"
    route:
      json_logs: '.container_name == "source-json-service"'
      apache_access_logs: '.container_name == "source-apache-access"'
      apache_error_logs: '.container_name == "source-apache-error"'
      rfc3164_syslogs: '.container_name == "source-rfc3164-syslog"'

  json_transform:
    inputs:
      - "route_logs.json_logs"
    type: "remap"
    source: |
      # Parse the nested JSON string
      parsed_message, err = parse_json(.message)

      if err == null {

        # Save metadata fields
        _container_name = .container_name
        _source_type = .source_type
        _stream = .stream
        _timestamp = .timestamp

        # The original logs
        . = parsed_message

        # Add back the metadata fields
        .container_name = _container_name
        .source_type = _source_type
        .stream = _stream

        # Parse timestamp
        ts, err = parse_timestamp(_timestamp, format: "%+")
        if err == null {
          .timestamp = format_timestamp!(ts, format: "%Y-%m-%d %H:%M:%S%.3f")
        } else {
          .timestamp = format_timestamp!(now(), format: "%Y-%m-%d %H:%M:%S%.3f")
        }
        

      } else {
        log("Failed to parse JSON in message", level: "error")
      }

  apache_access_transform:
    inputs:
      - "route_logs.apache_access_logs"
    type: "remap"
    source: |
      # Parse the nested JSON stringg
      parsed_message, err = parse_apache_log(.message, format: "combined")

      if err == null {

        # Save metadata fields
        _container_name = .container_name
        _source_type = .source_type
        _stream = .stream
        _timestamp = .timestamp
        
        # The original logs
        . = parsed_message

        # Add back the metadata fields
        .container_name = _container_name
        .source_type = _source_type
        .stream = _stream

        # Parse timestamp
        ts, err = parse_timestamp(_timestamp, format: "%+")
        if err == null {
          .timestamp = format_timestamp!(ts, format: "%Y-%m-%d %H:%M:%S%.3f")
        } else {
          .timestamp = format_timestamp!(now(), format: "%Y-%m-%d %H:%M:%S%.3f")
        }
      } else {
        log("Failed to parse Apache log", level: "error")
      }

  apache_error_transform:
    inputs:
      - "route_logs.apache_error_logs"
    type: "remap"
    source: |
      # Save metadata fields
      _container_name = .container_name
      _source_type = .source_type
      _stream = .stream
      _timestamp = .timestamp
      _raw_message = .message
      
      # Parse Apache error log format using regex
      # Format: [timestamp] [module:level] [pid tid] [client ip:port] message
      matches, err = parse_regex(_raw_message, r'^\[(?P<log_timestamp>[^\]]+)\] \[(?P<module_level>[^\]]+)\] \[pid (?P<pid>\d+):tid (?P<tid>\d+)\] \[client (?P<client>[^\]]+)\] (?P<error_message>.+)$')
      
      if err == null {
        # Set parsed fields
        .log_timestamp = matches.log_timestamp
        # Split module:level into separate fields
        parts = split!(matches.module_level, ":")
        .module = parts[0] 
        .level = parts[1]
        .pid = to_int!(matches.pid)
        .tid = to_int!(matches.tid)
        .client = matches.client
        .error_message = matches.error_message
        
        # Add back the metadata fields
        .container_name = _container_name
        .source_type = _source_type
        .stream = _stream

        # Remove unnecessary fields
        del(.message)
        del(.topic)
        del(.headers)
        del(.partition)
        del(.offset)
        del(.message_key)

        # Parse timestamp
        ts, err = parse_timestamp(_timestamp, format: "%+")
        if err == null {
          .timestamp = format_timestamp!(ts, format: "%Y-%m-%d %H:%M:%S%.3f")
        } else {
          .timestamp = format_timestamp!(now(), format: "%Y-%m-%d %H:%M:%S%.3f")
        }
      } else {
        log("Failed to parse Apache error log with regex", level: "error")
      }

  rfc3164_syslog_transform:
    inputs:
      - "route_logs.rfc3164_syslogs"
    type: "remap"
    source: |
      parsed_message, err = parse_syslog(.message)

      if err == null {
        # Save metadata fields
        _container_name = .container_name
        _source_type = .source_type
        _stream = .stream
        _timestamp = .timestamp

        # The original logs
        . = parsed_message

        # Add back the metadata fields
        .container_name = _container_name
        .source_type = _source_type
        .stream = _stream

        # Parse timestamp
        ts, err = parse_timestamp(_timestamp, format: "%+")
        if err == null {
          .timestamp = format_timestamp!(ts, format: "%Y-%m-%d %H:%M:%S%.3f")
        } else {
          .timestamp = format_timestamp!(now(), format: "%Y-%m-%d %H:%M:%S%.3f")
        }
        

      } else {
        log("Failed to parse RFC3164 syslog message", level: "error")
      }
sinks:
  clickhouse_json_logs:
    type: "clickhouse"
    inputs:
      - "json_transform"
    endpoint: "http://clickhouse:8123"
    database: "logs_db"
    table: "json_service_logs"
    auth:
      strategy: "basic"
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    compression: "gzip"
    batch:
      max_bytes: 104857600 
      max_events: 500000 
    buffer:
      type: "memory" 
      max_size: 100000
      when_full: "block" 
    healthcheck:
      enabled: true

  clickhouse_apache_access_logs:
    type: "clickhouse"
    inputs:
      - "apache_access_transform"
    endpoint: "http://clickhouse:8123"
    database: "logs_db"
    table: "apache_access_logs"
    auth:
      strategy: "basic"
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    compression: "gzip"
    batch:
      max_bytes: 104857600 
      max_events: 500000 
    buffer:
      type: "memory" 
      max_size: 100000
      when_full: "block" 
    healthcheck:
      enabled: true

  clickhouse_apache_error_logs:
    type: "clickhouse"
    inputs:
      - "apache_error_transform"
    endpoint: "http://clickhouse:8123"
    database: "logs_db"
    table: "apache_error_logs"
    auth:
      strategy: "basic"
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    compression: "gzip"
    batch:
      max_bytes: 104857600 
      max_events: 500000 
    buffer:
      type: "memory" 
      max_size: 100000
      when_full: "block" 
    healthcheck:
      enabled: true

  clickhouse_rfc3164_syslogs:
    type: "clickhouse"
    inputs:
      - "rfc3164_syslog_transform"
    endpoint: "http://clickhouse:8123"
    database: "logs_db"
    table: "rfc3164_syslogs"
    auth:
      strategy: "basic"
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    compression: "gzip"
    batch:
      max_bytes: 104857600 
      max_events: 500000 
    buffer:
      type: "memory" 
      max_size: 100000
      when_full: "block" 
    healthcheck:
      enabled: true
  prometheus:
    type: "prometheus_exporter"
    inputs:
      - "internal_metrics"
    address: "0.0.0.0:8687"
  # print_logs:
  #   type: console
  #   inputs:
  #     - "route_logs.apache_access_logs"
  #   encoding:
  #     codec: "json"
